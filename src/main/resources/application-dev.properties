### ⚙️ application.properties
#
## Application Configuration
#spring.application.name=llm-circuit-service
#server.port=8080
#server.servlet.context-path=/api/v1
#
## Logging Configuration
#logging.level.root=INFO
#logging.level.com.venu.llm.circuit=DEBUG
#logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} - %msg%n
#logging.file.name=logs/llm-service.log
#
## Jackson Configuration
#spring.jackson.serialization.indent-output=true
#spring.jackson.default-property-inclusion=non_null
#spring.jackson.serialization.write-dates-as-timestamps=false
#
## Actuator Configuration
#management.endpoints.web.exposure.include=health,info,metrics,prometheus
#management.endpoint.health.show-details=always
#management.metrics.export.prometheus.enabled=true
#
## OpenAI Configuration
#llm.openai.enabled=true
#llm.openai.api-key=${OPENAI_API_KEY:sk-your-api-key}
#llm.openai.api-url=https://api.openai.com/v1
#llm.openai.model=gpt-4
#llm.openai.temperature=0.7
#llm.openai.max-tokens=2000
#llm.openai.timeout=60000
#
## Google Gemini Configuration
#llm.gemini.enabled=true
#llm.gemini.api-key=${GEMINI_API_KEY:your-gemini-api-key}
#llm.gemini.api-url=https://generativelanguage.googleapis.com/v1
#llm.gemini.model=gemini-pro
#llm.gemini.temperature=0.7
#llm.gemini.max-tokens=2000
#llm.gemini.timeout=60000
#
## Anthropic Claude Configuration
#llm.claude.enabled=true
#llm.claude.api-key=${CLAUDE_API_KEY:your-claude-api-key}
#llm.claude.api-url=https://api.anthropic.com/v1
#llm.claude.model=claude-3-sonnet-20240229
#llm.claude.temperature=0.7
#llm.claude.max-tokens=2000
#llm.claude.timeout=60000
#
## Meta Llama Configuration (via Replicate or Together AI)
#llm.llama.enabled=true
#llm.llama.api-key=${LLAMA_API_KEY:your-llama-api-key}
#llm.llama.api-url=https://api.together.xyz/v1
#llm.llama.model=meta-llama/Llama-2-70b-chat-hf
#llm.llama.temperature=0.7
#llm.llama.max-tokens=2000
#llm.llama.timeout=60000
#
## Circuit Breaker Configuration
#resilience4j.circuitbreaker.instances.llm-service.slidingWindowSize=10
#resilience4j.circuitbreaker.instances.llm-service.minimumNumberOfCalls=5
#resilience4j.circuitbreaker.instances.llm-service.failureRateThreshold=50
#resilience4j.circuitbreaker.instances.llm-service.waitDurationInOpenState=30000
#resilience4j.circuitbreaker.instances.llm-service.permittedNumberOfCallsInHalfOpenState=3
#
## Retry Configuration
#resilience4j.retry.instances.llm-service.maxAttempts=3
#resilience4j.retry.instances.llm-service.waitDuration=1000
#resilience4j.retry.instances.llm-service.enableExponentialBackoff=true
#resilience4j.retry.instances.llm-service.exponentialBackoffMultiplier=2
#
## Rate Limiter Configuration
#resilience4j.ratelimiter.instances.llm-service.limitForPeriod=50
#resilience4j.ratelimiter.instances.llm-service.limitRefreshPeriod=60000
#resilience4j.ratelimiter.instances.llm-service.timeoutDuration=5000
#
## Thread Pool Configuration
#llm.executor.core-pool-size=10
#llm.executor.max-pool-size=50
#llm.executor.queue-capacity=100
#llm.executor.thread-name-prefix=llm-executor-
#
## Request Validation
#llm.validation.max-prompt-length=10000
#llm.validation.min-prompt-length=1